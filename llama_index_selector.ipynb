{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating different models from the Azure AI catalog with LlamaIndex\n",
    "\n",
    "The following notebook demonstrate how users can use multiple models from Azure AI studio depending on the scenario and use the right model for the right job. In this case, we will use LLamaIndex to build a RAG system and select different models from different providers, maximizing the capabilities they have on each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing\n",
    "\n",
    "In this example, we will use multiple models deployed in this project, including Phi-3, Cohere Command R+, Cohere Embed V3, Mistral Large, and OpenAI GPT-4o. Endpoints URLs and keys are stored in the `.env` file. Please update it accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure asynchronous operations on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure instrumentation\n",
    "\n",
    "We will use LlamaIndex to build a RAG system to answer different questions from the Paul Grahm dataset. To identify opportunities of improvement, we are using PromptFlow Tracing for tracing and monitoring. The following section configures automatic instrumentation of LlamaIndex and connects it with a PromptFlow server instance running locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "start_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure instrumentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrumentor = LlamaIndexInstrumentor()\n",
    "instrumentor.instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a RAG system with models from the catalog\n",
    "\n",
    "Let's use the Cohere model ecosystem to implement our RAG solution. Cohere models are optimized for RAG patterns and they can work in a large range of languages, specially when using the Cohere Embed V3 Multilingual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "from llama_index.llms.azure_inference import AzureAICompletionsModel\n",
    "from llama_index.embeddings.azure_inference import AzureAIEmbeddingsModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohere Command R+:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureAICompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_AI_COHERE_CMDR_ENDPOINT_URL\"],\n",
    "    credential=os.environ['AZURE_AI_COHERE_CMDR_ENDPOINT_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohere Embed V3 - Multilingual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = AzureAIEmbeddingsModel(\n",
    "    endpoint=os.environ[\"AZURE_AI_COHERE_EMBED_ENDPOINT_URL\"],\n",
    "    credential=os.environ['AZURE_AI_COHERE_EMBED_ENDPOINT_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the index\n",
    "\n",
    "To demostrate how to use different models, let's first create an index using Cohere models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use the Paul Graham dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data/paul_graham\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have documents, we create nodes by applying chunking into it as it is configured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = Settings.node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize storage context, by default it's in-memory so we don't have to worry about persisting them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RAG system will be able to answer questions that look to summarize multiple sources of information vs a more simple retrieval strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary index is a simple data structure where nodes are stored in a sequence. During index construction, the document texts are chunked up, converted to nodes, and stored in a list. During query time, the summary index iterates through the nodes with some optional filter parameters, and synthesizes an answer from all the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_index = SummaryIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VectorStoreIndex` only stores nodes in document store if vector store does not store text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing the query engine with the search tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summarize_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to Paul Graham eassy on\"\n",
    "        \" What I Worked On.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from Paul Graham essay on What\"\n",
    "        \" I Worked On.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is a collection of essays by Paul Graham, detailing his journey as a writer, programmer, entrepreneur, and investor. Graham reflects on his early experiences with writing and programming, his transition into the field of artificial intelligence (AI) during his college years, and his realization of the limitations of AI as it was practiced at the time. He discusses his interest in the Lisp programming language and his desire to build something that would last, which leads him to pursue art and attend art school. Graham also shares his experiences working at Interleaf, a software company, and his subsequent return to art school. He then narrates the founding of his first company, Viaweb, an e-commerce software startup, and the challenges and successes he faced as an entrepreneur. After selling Viaweb to Yahoo, Graham focuses on open-source projects, including a new dialect of Lisp called Arc, and continues to publish essays online. He also meets his future partner, Jessica Livingston, and together they co-found Y Combinator, a successful startup investment and mentoring firm. Graham reflects on the early days of Y Combinator and his decision to eventually step down as its president. The essays conclude with Graham's thoughts on various topics, including the concept of invention versus discovery, the influence of customs and rapid change in different fields, and his satisfaction with his chosen path.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After leaving RICS, Paul Graham returned to New York and resumed his previous life, but with the added financial freedom from his work at RICS. He continued to paint and experiment with new techniques, and also began searching for a new apartment to buy. During this time, he had an idea for a new startup, which would eventually lead to the creation of Y Combinator.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did Paul Graham do after RICS?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an smaller model for simpler tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using exactly the same class `AzureAIModelInferenceLLM` we can instantiate another model, in this case a Phi-3-mini-4K model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "slm = AzureAICompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_AI_PHI3_MINI_ENDPOINT_URL\"],\n",
    "    credential=os.environ['AZURE_AI_PHI3_MINI_ENDPOINT_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's configure the `RouterQueryEngine` to use Phi-3 for the routing task instead of the larger model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(llm=slm),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After leaving RICS, Paul Graham returned to New York and resumed his previous life, but with the added financial freedom from his work at RICS. He continued to paint and experiment with new techniques, combining traditional painting with photography and printmaking. He also began looking for a new apartment to buy, contemplating which neighborhood would be the best fit for him. During this time, he had an idea for a new startup, which would eventually lead to the creation of a new company and the development of several open-source software projects.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did Paul Graham do after RICS?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an evaluation dataset\n",
    "\n",
    "Let's build an evaluation dataset to see the effect of the change in the model. We will use another LLM to generate examples, in this case Mistral Large which is a good model for RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_llm = AzureAICompletionsModel(\n",
    "    endpoint=os.environ[\"AZURE_AI_MISTRAL_ENDPOINT_URL\"],\n",
    "    credential=os.environ[\"AZURE_AI_MISTRAL_ENDPOINT_KEY\"],\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import LabelledRagDataset\n",
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = RagDatasetGenerator.from_documents(\n",
    "    documents=documents,\n",
    "    llm=generator_llm,\n",
    "    num_questions_per_chunk=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:00<00:00, 1214140.63it/s]\n"
     ]
    }
   ],
   "source": [
    "rag_dataset = dataset_generator.generate_questions_from_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: In the context of the author's early programming experiences, what were the limitations of the IBM 1401 mainframe computer that hindered his ability to write meaningful programs?\n",
      "Context: What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college th ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Query:\", rag_dataset[1].query)\n",
    "print(\"Context:\", rag_dataset[1].reference_contexts[0][:50], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_dataset.save_json(\"evals/pg_rag_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reload them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_dataset = LabelledRagDataset.from_json(\"evals/pg_rag_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use evaluations for retrieval\n",
    "\n",
    "__FaithfulnessEvaluator__\n",
    "\n",
    "`FaithfulnessEvaluator` is used to measure if the response from a query engine matches any response nodes. This is useful for measuring if the response has hallucinated.\n",
    "\n",
    "__RelevancyEvaluator__\n",
    "\n",
    "`RelevancyEvaluator` is used to measure if the response and the source nodes match the query. This is useful for measuring if the query was actually answered by the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "from llama_index.core.evaluation import RelevancyEvaluator, FaithfulnessEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case let's use a more powerful model as a judge, being GPT-4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4judge = AzureOpenAI(\n",
    "    deployment=\"gpt-4\",\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_KEY\"],\n",
    "    api_version=\"2023-07-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the evaluators with this LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancy_evaluator = RelevancyEvaluator(llm=gpt4judge)\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(llm=gpt4judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset of the `query` property only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_eval_queries = [sample.query for sample in rag_dataset[1::2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `rag_dataset[1::2]` retries the odd indexes only, since the dataset contains \"question 1:\" as part of the generation. It probably requires to change the generation template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `BatchEvalRunner` will allow us to run evalutions over all the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = BatchEvalRunner(\n",
    "    {\n",
    "        \"faithfulness\": faithfulness_evaluator,\n",
    "        \"relevancy\": relevancy_evaluator\n",
    "    },\n",
    "    workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = await runner.aevaluate_queries(\n",
    "    query_engine, queries=batch_eval_queries\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "eval_results_dict = {}\n",
    "eval_results_dict[\"faithfulness\"] = [\n",
    "    dict(result) for result in eval_results[\"faithfulness\"]]\n",
    "eval_results_dict[\"relevancy\"] = [\n",
    "    dict(result) for result in eval_results[\"relevancy\"]]\n",
    "\n",
    "with open(\"evals/pg_rag_eval_results_phi3.json\", \"w\") as f:\n",
    "    json.dump(eval_results_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_score = sum(\n",
    "    result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
    "relevancy_score = sum(\n",
    "    result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score: 0.9696969696969697\n",
      "Relevancy Score: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "print(f\"Faithfulness Score: {faithfulness_score}\")\n",
    "print(f\"Relevancy Score: {relevancy_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
